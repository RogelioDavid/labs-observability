kind: ConfigMap
metadata:
  name: grafana-alloy
  namespace: monitoring
apiVersion: v1
data:
  config.alloy: |
    logging {
        level  = "debug"
        format = "json"
    }

    // Prometheus configs
    prometheus.remote_write "metrics_hello_observability" {
      external_labels = {
        monitor = "hello-observability",
        cluster = "autopilot-cluster-app",
      }
      endpoint {
        name = "hello-observability-6397c4"
      url  = "https://prometheus-us-central1.grafana.net/api/prom/push"
      basic_auth {
        username = "821057"
        password = "eyJrIjoiNGRjNDU3MmQ1MTVlOTk2ODQ1Zjk1MGNmZDcwNGQyMmIwOGEwNjA3MyIsIm4iOiJyb2dtZW4tcHVibGlzaGVyIiwiaWQiOjgxNDMyMn0="
      }
      }
    }
    discovery.relabel "metrics_hello_observability_ms_service_app_ms1" {
      targets = [{
        __address__ = "ms-service-app-ms1.applications.svc.cluster.local:8081",
      }]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    discovery.relabel "metrics_hello_observability_ms_service_app_ms2" {
      targets = [{
        __address__ = "ms-service-app-ms2.applications.svc.cluster.local:8082",
      }]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    discovery.relabel "metrics_hello_observability_hello_observability" {
      targets = [{
        __address__ = "hello-observability.applications.svc.cluster.local:8080",
      }]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    prometheus.scrape "metrics_hello_observability_ms_service_app_ms1" {
      targets         = discovery.relabel.metrics_hello_observability_ms_service_app_ms1.output
      forward_to      = [prometheus.relabel.metrics_hello_observability_ms_service_app_ms1.receiver]
      job_name        = "ms-service-app-ms1"
      scrape_interval = "10s"
    }

    prometheus.scrape "metrics_hello_observability_ms_service_app_ms2" {
      targets         = discovery.relabel.metrics_hello_observability_ms_service_app_ms2.output
      forward_to      = [prometheus.relabel.metrics_hello_observability_ms_service_app_ms2.receiver]
      job_name        = "ms-service-app-ms2"
      scrape_interval = "10s"
    }

    prometheus.scrape "metrics_hello_observability_hello_observability" {
      targets         = discovery.relabel.metrics_hello_observability_hello_observability.output
      forward_to      = [prometheus.relabel.metrics_hello_observability_hello_observability.receiver]
      job_name        = "hello-observability"
      scrape_interval = "10s"
    }

    prometheus.relabel "metrics_hello_observability_ms_service_app_ms1" {
      forward_to = [prometheus.remote_write.metrics_hello_observability.receiver]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    prometheus.relabel "metrics_hello_observability_ms_service_app_ms2" {
      forward_to = [prometheus.remote_write.metrics_hello_observability.receiver]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    prometheus.relabel "metrics_hello_observability_hello_observability" {
      forward_to = [prometheus.remote_write.metrics_hello_observability.receiver]
      rule {
        source_labels = ["trace_id"]
        target_label  = "traceID"
      }
    }

    // LOKI configs
    loki.write "logs_default" {
      endpoint {
        url = "https://logs-prod-017.grafana.net/loki/api/v1/push"

        basic_auth {
          username = "409497"
          password = "eyJrIjoiNGRjNDU3MmQ1MTVlOTk2ODQ1Zjk1MGNmZDcwNGQyMmIwOGEwNjA3MyIsIm4iOiJyb2dtZW4tcHVibGlzaGVyIiwiaWQiOjgxNDMyMn0="
        }
      }
      external_labels = {
        cluster = "autopilot-cluster-app",
      }
    }

    local.file_match "logs_default_hello_observability" {
      path_targets = [{
        __address__ = "localhost",
        __path__    = "/tmp/hello-observability.log",
        job         = "hello-observability",
      }]
    }

    local.file_match "logs_default_ms_service_app_ms1" {
      path_targets = [{
        __address__ = "localhost",
        __path__    = "/tmp/ms-service-app-ms1.log",
        job         = "ms-service-app-ms1",
      }]
    }

    local.file_match "logs_default_ms_service_app_ms2" {
      path_targets = [{
        __address__ = "localhost",
        __path__    = "/tmp/ms-service-app-ms2.log",
        job         = "ms-service-app-ms2",
      }]
    }

    loki.source.file "logs_default_hello_observability" {
      targets    = local.file_match.logs_default_hello_observability.targets
      forward_to = [loki.write.logs_default.receiver]
    }


    loki.source.file "logs_default_ms_service_app_ms1" {
      targets    = local.file_match.logs_default_ms_service_app_ms1.targets
      forward_to = [loki.write.logs_default.receiver]
    }

    loki.source.file "logs_default_ms_service_app_ms2" {
      targets    = local.file_match.logs_default_ms_service_app_ms2.targets
      forward_to = [loki.write.logs_default.receiver]
    }

    local.file_match "logs_default_tomcat_access" {
      path_targets = [{
        __address__ = "localhost",
        __path__    = "/tmp/access_log.log",
        job         = "tomcat-access",
      }]
    }

    loki.source.file "logs_default_tomcat_access" {
      targets    = local.file_match.logs_default_tomcat_access.targets
      forward_to = [loki.write.logs_default.receiver]
    }


    discovery.kubernetes "logs_integrations_integrations_grafana_mimir_logs" {
      role = "pod"
    }

    discovery.relabel "logs_integrations_integrations_grafana_mimir_logs" {
      targets = discovery.kubernetes.logs_integrations_integrations_grafana_mimir_logs.targets

      rule {
        source_labels = ["__meta_kubernetes_pod_label_helm_sh_chart"]
        regex         = "mimir-distributed-.*"
        action        = "keep"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label  = "__host__"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator     = "/"
        target_label  = "job"
      }

      rule {
        source_labels = ["cluster"]
        separator     = ""
        regex         = ""
        target_label  = "cluster"
        replacement   = "k8s-cluster"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "name"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator     = "/"
        target_label  = "__path__"
        replacement   = "/var/log/pods/*$1/*.log"
      }
    }

    local.file_match "logs_integrations_integrations_grafana_mimir_logs" {
      path_targets = discovery.relabel.logs_integrations_integrations_grafana_mimir_logs.output
    }

    loki.process "logs_integrations_integrations_grafana_mimir_logs" {
      forward_to = [loki.write.logs_default.receiver]

      stage.cri { }
    }

    loki.source.file "logs_integrations_integrations_grafana_mimir_logs" {
      targets    = local.file_match.logs_integrations_integrations_grafana_mimir_logs.targets
      forward_to = [loki.process.logs_integrations_integrations_grafana_mimir_logs.receiver]
    }

    discovery.kubernetes "logs_integrations_integrations_kubernetes_pod_logs" {
      role = "pod"
    }

    discovery.relabel "logs_integrations_integrations_kubernetes_pod_logs" {
      targets = discovery.kubernetes.logs_integrations_integrations_kubernetes_pod_logs.targets

      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label  = "__host__"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
        separator     = "/"
        target_label  = "job"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator     = "/"
        target_label  = "__path__"
        replacement   = "/var/log/pods/*$1/*.log"
      }
    }

    local.file_match "logs_integrations_integrations_kubernetes_pod_logs" {
      path_targets = discovery.relabel.logs_integrations_integrations_kubernetes_pod_logs.output
    }

    loki.process "logs_integrations_integrations_kubernetes_pod_logs" {
      forward_to = [loki.write.logs_default.receiver]

      stage.docker { }
    }

    loki.source.file "logs_integrations_integrations_kubernetes_pod_logs" {
      targets    = local.file_match.logs_integrations_integrations_kubernetes_pod_logs.targets
      forward_to = [loki.process.logs_integrations_integrations_kubernetes_pod_logs.receiver]
    }



    // OtelCol configs

    otelcol.auth.basic "tempo" {
      username = 406010
      password = "eyJrIjoiNGRjNDU3MmQ1MTVlOTk2ODQ1Zjk1MGNmZDcwNGQyMmIwOGEwNjA3MyIsIm4iOiJyb2dtZW4tcHVibGlzaGVyIiwiaWQiOjgxNDMyMn0="
    }
    otelcol.auth.basic "promp" {
      username = 406010
      password ="eyJrIjoiNGRjNDU3MmQ1MTVlOTk2ODQ1Zjk1MGNmZDcwNGQyMmIwOGEwNjA3MyIsIm4iOiJyb2dtZW4tcHVibGlzaGVyIiwiaWQiOjgxNDMyMn0="
    }

    otelcol.receiver.otlp "receiverbase" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      output {
        metrics = [otelcol.processor.batch.processorbatch.input]
        logs    = [otelcol.processor.batch.processorbatch.input]
        traces  = [otelcol.processor.memory_limiter.memory_limiter.input]
      }
    }

    otelcol.processor.memory_limiter "memory_limiter" {
      check_interval = "5s"
      limit = "400MiB"        // Hard Limit
      spike_limit = "100MiB"  // Soft Limit = Hard Limit - spike_limit = 300MiB
      output {
        traces  = [otelcol.processor.filter.default.input]
      }
    } 

    otelcol.processor.filter "default" {
      error_mode = "ignore"
      traces {
        span = [
          "IsMatch(attributes[\"http.route\"], \".*prometheus.*\")",
          "IsMatch(attributes[\"http.route\"], \".*health.*\")",
          "IsMatch(attributes[\"http.target\"], \".*health.*\")",
          "IsMatch(attributes[\"http.target\"], \".*health_check.*\")",
          "IsMatch(attributes[\"db.operation\"], \".*PING.*\")",
          "IsMatch(attributes[\"express.type\"], \".*middleware.*\")",
          "IsMatch(attributes[\"express.type\"], \".*router.*\")",
          "IsMatch(attributes[\"db.operation\"], \".*EXIST.*\")",
          "IsMatch(attributes[\"express.name\"], \".*request handler.*\")",
          "IsMatch(attributes[\"net.peer.port\"], \".*9000.*\")",
          "IsMatch(attributes[\"db.system\"], \".*redis.*\")",
        ]
      }

      output {
        traces  = [otelcol.processor.batch.processorbatch.input]
      }
    }


    prometheus.exporter.self "self_metrics" {}

    prometheus.scrape "scrape_self" {
      targets    = prometheus.exporter.self.self_metrics.targets
      forward_to = [otelcol.receiver.prometheus.prom_to_otlp.receiver]
    }

    otelcol.receiver.prometheus "prom_to_otlp" {
      output {
        metrics = [otelcol.processor.batch.processorbatch.input]
      }
    }
    otelcol.processor.batch "processorbatch" {
      timeout = "10s"
      send_batch_size = 10000
      send_batch_max_size = 11000
      output {
        metrics = [otelcol.exporter.otlp.basetotlp.input]
        logs    = [otelcol.exporter.otlp.basetotlp.input]
        traces  = [otelcol.exporter.otlp.basetotlp.input]
      }
    }


    otelcol.exporter.prometheus "default" {
      forward_to = [prometheus.remote_write.metrics_hello_observability.receiver]
    }

    otelcol.exporter.otlp "basetotlp" {
      client {
        endpoint = "tempo-us-central1.grafana.net:443"
        auth     = otelcol.auth.basic.tempo.handler
      }
    }
    otelcol.exporter.logging "defaultlogging" {
      verbosity = "normal"
    }